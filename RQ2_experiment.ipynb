{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da7092c-4f47-455f-b3ac-5c0079ddee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:43:35,108 - modelscope - INFO - PyTorch version 2.1.1+cu121 Found.\n",
      "2024-12-11 21:43:35,109 - modelscope - INFO - Loading ast index from C:\\Users\\Administrator\\.cache\\modelscope\\ast_indexer\n",
      "2024-12-11 21:43:35,143 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 256e9fa31dca7b4f62f9ab98dddffdae and a total number of 946 components indexed\n"
     ]
    }
   ],
   "source": [
    "from modelscope.utils.hf_util import AutoModelForCausalLM, AutoTokenizer,snapshot_download\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Whether the model has loaded.\n",
    "start = 0\n",
    "\n",
    "# We strongly recommend using independently python environment on each LLM.\n",
    "model_dir_list = [\"D:\\Jupyter_Code\\model\\Yi-34B-Chat-4bits\", \n",
    "                  \"D:\\Jupyter_Code\\model\\Yi-6B\",\n",
    "                  \"D:\\Jupyter_Code\\model\\Phi-3-mini-4k-instruct\",\n",
    "                  \"D:\\Jupyter_Code\\model\\LLM-Research\\Llama-3.2-3B\",\n",
    "                  \"D:\\\\Jupyter_Code\\\\model\\\\Qwen-14B-Chat-Int8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1685b1-04f9-4aed-bc2f-04d8ac3e3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model in model_dir_list will be used.\n",
    "model_index = 4\n",
    "# path of choice list.\n",
    "choice_path = \"dataset\\\\RQ2_dataset\\\\choice\\\\\"\n",
    "# path of test target.\n",
    "task_path = \"dataset\\\\RQ2_dataset\\\\task\\\\\"\n",
    "# path of the prompt.\n",
    "prompt_path = \"dataset\\\\RQ2_dataset\\\\prompt\\\\\"\n",
    "# Your textual and selection index result will be saved in this folder.\n",
    "result_path = \"result\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed28552a-136a-4083-8280-2de48794606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yi_text_generation(input, max_new_tokens = 256, temperature = 1, top_k = 3, do_sample = True, print = True):\n",
    "    '''\n",
    "    This function returns the string result of LLM.\n",
    "    We package this LLM into one function. Just call this function, and it will give the result(response).\n",
    "    There are three models: Yi-6B, Yi-34B(cannnot run in this computer), and Yi-34B-Chat-4bits.\n",
    "    The format of input depends on your mode. If mode is 0, input should be a str which the model will generate the text after it;\n",
    "    if mode is 1, input should be the format of chat. We highly recommend you use mode 0.\n",
    "    Other parameters are used to give LLM. \n",
    "    Parameters maybe not important when using GPT-4o, but it is important when using open-source LLM. \n",
    "    The best parameter depends on your task. You may try some times to get the optimum.\n",
    "    To make it more convenient to use, we give the default parameters. You can set it when you use this function.\n",
    "\n",
    "    '''\n",
    "    global start, model_dir, model, tokenizer, model_index\n",
    "\n",
    "    if start == 0:\n",
    "        start = 1\n",
    "        model_dir = model_dir_list[model_index]\n",
    "        if model_index == 0:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda\", torch_dtype=\"auto\",\n",
    "                                                         offload_folder=\"offload_folder\", trust_remote_code=True)\n",
    "        elif model_index == 1:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda\", trust_remote_code=True)\n",
    "        elif model_index == 2:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "        elif model_index == 3:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True,)\n",
    "        elif model_index == 4:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True,)\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, download_path = download_path)\n",
    "    model_inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    inputs = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(inputs.shape, dtype = torch.long, device=\"cuda\")\n",
    "    outputs = model.generate(model_inputs.input_ids.cuda(), \n",
    "                             temperature = temperature,\n",
    "                             top_k = top_k,\n",
    "                             do_sample = do_sample,\n",
    "                             max_new_tokens = max_new_tokens,\n",
    "                             attention_mask = attention_mask,\n",
    "                             pad_token_id = tokenizer.eos_token_id\n",
    "                             )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if print == True:\n",
    "        print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8db93b0-4290-4b56-ae8e-4135df268e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(str_return, choice_list):\n",
    "    # Compare result of LLM and the choice_list and ground truth; return the index of selected choice.\n",
    "    # find_id is the return value; 0 means fail to select any widget.\n",
    "    find_id = 0\n",
    "    # find_pos means the match position in the return string of LLM, aim at to find the first one matched. \n",
    "    find_pos = len(str_return) + 1\n",
    "    # We try to match the first occurrence of a valid widget name\n",
    "    ad = 1\n",
    "    for choice in choice_list:\n",
    "        choice = choice[choice.find(\":\") + 2:]\n",
    "        now_pos = str_return.find(choice)\n",
    "        if now_pos >= 0 and now_pos < find_pos:\n",
    "            find_id = ad\n",
    "            find_pos = now_pos\n",
    "        ad = ad + 1\n",
    "    # If find_id is not zero, return the value.\n",
    "    if find_id != 0:\n",
    "        return find_id\n",
    "    # Else we try to match the index.\n",
    "    for i in range(len(choice_list)):\n",
    "        str_com = str(i + 1)\n",
    "        now_pos = str_return.find(choice)\n",
    "        if now_pos >= 0 and now_pos < find_pos:\n",
    "            find_id = ad\n",
    "            find_pos = now_pos\n",
    "        ad = ad + 1\n",
    "    # Whether success or not, return the find_id.\n",
    "    return find_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9d62d5-e300-4c14-b3ce-f15109d435f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_text_generation(file_str, std_str, num_res, print_step = True, cost_time = 0):\n",
    "    choice_list = []\n",
    "    choice_name = choice_path + \"choice\" + std_str + \".txt\"\n",
    "    with open(choice_name, 'r') as file: \n",
    "        line = file.readline() \n",
    "        while line: \n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            choice_list.append(line)\n",
    "            line = file.readline()\n",
    "    # test target and ground truth are saved in task_path.\n",
    "    np_task = np.array(pd.read_csv(task_path + \"task\" + std_str + \".csv\"))\n",
    "    np_result = np.zeros((5, 12))\n",
    "    np_time = np.zeros((5, 12))\n",
    "    np_score = np.zeros(12)\n",
    "    start_time = time.time()\n",
    "    record_time = time.time()\n",
    "    \n",
    "\n",
    "    if print_step == True:\n",
    "        print(\"start\", end = \"...\")\n",
    "    str_filename = prompt_path + file_str\n",
    "    message = \"\"\n",
    "    with open(str_filename, 'r', encoding = \"UTF-8\") as f:\n",
    "        message = f.read()\n",
    "    for topk in range(2, 5):\n",
    "        for temp in range(7, 11):\n",
    "            np_id = (topk - 2) * 4 + (temp - 7)\n",
    "            for repeat in range(0, 5):\n",
    "                str_return = Yi_text_generation(message, max_new_tokens = 128, temperature = temp / 10, top_k = topk, print = False)\n",
    "                str_return = str_return[str_return.index(\"###\"):]\n",
    "                with open (result_path + \"result\" + std_str + \".txt\", \"a+\", encoding = \"UTF-8\") as f:\n",
    "                    f.write(\"###Answer top_k = \" + str(topk) + \", temperature = \" + str(temp / 10) + \", repeat time = \" + str(repeat + 1) + \"\\n\")\n",
    "                    f.write(str_return)\n",
    "                    f.write(\"\\n###Answer end\\n\")\n",
    "                comp_res = compare(str_return, choice_list)\n",
    "                np_result[repeat][np_id] = comp_res\n",
    "                mid_time = time.time()\n",
    "                np_time[repeat][np_id] = mid_time - record_time\n",
    "                record_time = mid_time\n",
    "                if comp_res == int(np_task[num_res][1]):\n",
    "                    np_score[np_id] = np_score[np_id] + 0\n",
    "                else:\n",
    "                    np_score[np_id] = np_score[np_id] + 0.2\n",
    "    if print_step == True:\n",
    "        now_time = time.time()\n",
    "        print(\"end, time cost: \", str(now_time - start_time), \" seconds\", end = \"\\n\")\n",
    "            \n",
    "    pd_result = pd.DataFrame(np_result)\n",
    "    pd_score = pd.DataFrame(np_score)\n",
    "    pd_time = pd.DataFrame(np_time)\n",
    "    pd_result.to_csv(result_path + \"result\" + std_str + \"_\" + str(num_res) + \".csv\")\n",
    "    pd_score.to_csv(result_path + \"score\" + std_str + \"_\" + str(num_res) + \".csv\")\n",
    "    pd_time.to_csv(result_path + \"time\" + std_str + \"_\" + str(num_res) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce64e09-8544-4ad2-97e1-ed03656c3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_files = os.listdir(choice_path)\n",
    "# Given the lengthy experimental procedure, this variable allows resuming the experiment from the last interruption point.\n",
    "result_files = os.listdir(result_path)\n",
    "for choice_file in choice_files:\n",
    "    num_pat = re.compile(r\"[0-9]+\")\n",
    "    num_res = int(num_pat.findall(choice_file)[-1])\n",
    "    #print(num_res)\n",
    "    std_file_str = re.sub(r\"[0-9]+.txt\", \"\", choice_file)\n",
    "    std_file_str = std_file_str.replace(\"LLM_prompt\", \"\")\n",
    "    #print(choice_file, std_file_str)\n",
    "\n",
    "    find_file = 0\n",
    "    for result_file in result_files:\n",
    "        if result_file.find(\".csv\") == -1:\n",
    "            continue\n",
    "        if result_file.find(std_file_str + \"_\" + str(num_res)) != -1:\n",
    "            find_file = 1\n",
    "    if find_file == 1:\n",
    "        continue\n",
    "\n",
    "    print(choice_file, \",\", end = \"\")\n",
    "    group_text_generation(choice_file, std_file_str, num_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc20f4-100c-4922-90a0-1bb3e4463442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
